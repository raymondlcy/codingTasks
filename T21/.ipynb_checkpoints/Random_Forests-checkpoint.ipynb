{"cells":[{"cell_type":"markdown","source":["# Random Forests"],"metadata":{"id":"IRPrKgqfaZJd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN9VMGrB7f2S"},"outputs":[],"source":["# import packages\n","import pandas as pd\n","\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Create toy data\n","X, y = make_classification(n_samples=1000, n_features=10,\n","                           n_informative=5, n_redundant=0,\n","                           random_state=123, shuffle=False)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmYXIFFc7f2Y","outputId":"a462baff-b425-4eba-f2a4-0419e5932a6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy base: 0.92\n","Accuracy ensemble: 0.9433333333333334\n"]}],"source":["# Create base model\n","base = DecisionTreeClassifier(max_depth=5)\n","ensemble = BaggingClassifier(base_estimator=base, n_estimators=100, random_state=7)\n","\n","base.fit(X_train,y_train)\n","ensemble.fit(X_train,y_train)\n","\n","print(\"Accuracy base:\",base.score(X_test, y_test))\n","print(\"Accuracy ensemble:\",ensemble.score(X_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"xV_ujLdW7f2b"},"source":["### Importance scores\n","\n","A property of the Random Forest ensemble method in sklearn is that they let you print importance scores for features in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMD73jvh7f2c","outputId":"93459f52-cad5-4e67-f5fa-bedf1daa79bf"},"outputs":[{"data":{"text/plain":["0    0.361927\n","1    0.264725\n","2    0.121329\n","3    0.092373\n","4    0.053309\n","9    0.024595\n","7    0.023570\n","8    0.023236\n","5    0.017681\n","6    0.017256\n","dtype: float64"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["forest = RandomForestClassifier(n_estimators=100, random_state=7)\n","forest.fit(X_train, y_train)\n","\n","feature_imp = pd.Series(forest.feature_importances_).sort_values(ascending=False)\n","feature_imp"]},{"cell_type":"markdown","metadata":{"id":"DicM0dwn7f2d"},"source":["At the start of this notebook, we specified that this dataset has 10 features, of which 5 are informative. The classifier indeed relied on the bottom five features less than on the top. An advantage of investigating the importance of features is that irrelevant features can be removed. This removal of noise tends to improve performance and reduce training time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YXzZrgj7f2e","outputId":"6f113689-3072-4432-8dca-ec1b7c645c9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 10)\n","(1000, 5)\n","Accuracy base: 0.93\n","Accuracy ensemble: 0.9566666666666667\n"]}],"source":["# select important features\n","X = X[:, :5]\n","\n","# retrain\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n","base.fit(X_train,y_train)\n","ensemble.fit(X_train,y_train)\n","\n","print(\"Accuracy base:\",base.score(X_test, y_test))\n","print(\"Accuracy ensemble:\",ensemble.score(X_test, y_test))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}